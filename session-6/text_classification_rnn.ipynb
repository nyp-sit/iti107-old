{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-6/text_classification_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SN5USFEIIK3"
      },
      "source": [
        "# Text Classification using RNN\n",
        "In this lab exercise, we will learn to use LSTM (an RNN variant) to train a model to classify a piece of text as expressing positive sentiment or negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUQErGewZxE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFctV8-JZOc"
      },
      "source": [
        "### Download the IMDb Dataset\n",
        "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). You will train a sentiment classifier model on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPO4_UmfF0KH"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6yROZNKvbd"
      },
      "source": [
        "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iOHJGN6SDu"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O59BdioK8jY"
      },
      "source": [
        "The `train` directory also has additional folders which should be removed before creating training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Vfi9oWMSh-"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFoJjiEyJz9u"
      },
      "source": [
        "Next, create a `tf.data.Dataset` using `tf.keras.preprocessing.text_dataset_from_directory`. You can read more about this utility from the [api documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory). \n",
        "\n",
        "Use the `train` directory to create both train and validation datasets with a split of 20% for validation. Also note that here we use a smaller batch size of 128, as our model now is more complex, and will use up some significant memory, leaving little room for larger batch size.\n",
        "\n",
        "***Important note***\n",
        "\n",
        "Note: When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItYD3TLkCOP1"
      },
      "source": [
        "batch_size = 128\n",
        "seed = 123\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='training', seed=seed)\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='validation', seed=seed)\n",
        "test_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa6cq0-Ym0g"
      },
      "source": [
        "Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTCbSkvkYmTT"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz6k1IW7h1TO"
      },
      "source": [
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGicgV5qT0wh"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6NZSqIIoU0Y"
      },
      "source": [
        "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. \n",
        "\n",
        "TextVectorization layer is a text tokenizer which breaks up the text into words (it is similar to Keras Tokenizer but implemented as a layer). You can read more about TextVectorization layer [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
        "\n",
        "There are two ways to use TextVectorization: \n",
        "1. as part of the `tf.data` pipeline\n",
        "2. as part of the model (i.e. as a layer in the model)\n",
        "\n",
        "If you are doing training on GPU, it is better to use option 1 as it allows you to do asynchronous preprocessing of your data on CPU (because text vectorization does not use GPU), while GPU runs the model on one batch of data.This will lead to better training throughput. \n",
        "\n",
        "However, if you are exporting the model for inference in the production environment, you want to package the TextVectorization layer as part of the model, to make entire model self-contained without having to deploy additional preprocessing codes.\n",
        "\n",
        "In the code below, we will use option 1 for pre-processing text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlsXzo-ZlfK"
      },
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to \n",
        "# integers.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, \n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsxTnJSv2kF_"
      },
      "source": [
        "print(len(vectorize_layer.get_vocabulary()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WFdR5980Hvj"
      },
      "source": [
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (vectorize_layer(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (vectorize_layer(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (vectorize_layer(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI9_wLIiWO8Z"
      },
      "source": [
        "## Create a classification model\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/it3103/bidirectionalRNN.png\"/>\n",
        "\n",
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be built as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the vectorization layer, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the vectorization layer is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. We use a masking layer to mask off those padded positions so that the padded positions will not be used in the computation of loss. \n",
        "\n",
        "5. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "6. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n",
        "\n",
        "**NOTE**: Step 4 is skipped to avoid model loading error when mask=True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHLcFtn5Wsqj"
      },
      "source": [
        "EMBEDDING_DIM=128\n",
        "\n",
        "inputs = keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "## setting mask_zero to True does not work with TF2.7, as it causes the load_model() to throw exception\n",
        "# embedded = keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
        "#                                   output_dim=EMBEDDING_DIM, \n",
        "#                                   mask_zero=True)(inputs)\n",
        "embedded = keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
        "                                  output_dim=EMBEDDING_DIM)(inputs)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(embedded)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x = keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLNgKO7W2fe"
      },
      "source": [
        "## Compile and train the model\n",
        "\n",
        "We will use the model_checkpoint_callback to save our best checkpoint in terms of validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Hg3IHFt4Px"
      },
      "source": [
        "def save_best_model(checkpoint_path): \n",
        "\n",
        "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    \n",
        "    return model_checkpoint_callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrKAKAKIbuH"
      },
      "source": [
        "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCUgdP69Wzix"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mQehiQyv8rP"
      },
      "source": [
        "model.fit(\n",
        "    int_train_ds, \n",
        "    validation_data=int_val_ds,\n",
        "    epochs=3, \n",
        "    callbacks=[save_best_model('best_checkpoint_1_bilstm')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wYnVedSPfmX"
      },
      "source": [
        "The model reaches a validation accuracy of around 85% after 1 epoch of training.\n",
        "\n",
        "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTJMGvvR6mTs"
      },
      "source": [
        "Let's evaluate the model on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk-tEK086qko"
      },
      "source": [
        "model.load_weights(\"best_checkpoint_1_bilstm\")\n",
        "model.evaluate(int_test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzXFaQSIkdjJ"
      },
      "source": [
        "## Prepare Model for Deployment \n",
        "\n",
        "As mentioned earlier, it is better to package the TextVectorization layer as part of the model for ease of deployment, so that we can run the raw text directly through the model during inference.\n",
        "\n",
        "In the code below, we declare a Input layer that takes in a string (shape=(1,)), and we add the Text Vectorization layer, and then stick them to our previous model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyn9rYk042RO"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = vectorize_layer(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzSEgryN1_1B"
      },
      "source": [
        "\n",
        "Let's go ahead and save our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnKLmU_ksqk"
      },
      "source": [
        "inference_model.save('sentiment_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtek2H0A2TWK"
      },
      "source": [
        "Now let us put our model in use!!  We will first load our saved model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1DJOzal2yQl"
      },
      "source": [
        "loaded_model = keras.models.load_model('sentiment_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edrL9YQ03Tqb"
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLaU3AKb3T7v"
      },
      "source": [
        "Run the following cell and type in your own text at the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBr_A1W2h6F"
      },
      "source": [
        "text = input(\"Write your review here:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR-ZsF345rSU"
      },
      "source": [
        "text = tf.convert_to_tensor(text)\n",
        "text = tf.expand_dims(text, axis=0)\n",
        "pred = loaded_model(text)[0]\n",
        "print(pred)\n",
        "if pred >= 0.5: \n",
        "    print('positive sentiment')\n",
        "else:\n",
        "    print('negative sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c570Kdh-kmYZ"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/it3103/layered_bidirectional.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29uXYt5gkuW8"
      },
      "source": [
        "inputs = keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "embedded = keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
        "                                  output_dim=EMBEDDING_DIM,\n",
        "                                  name='embedding')(inputs)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(64,  return_sequences=True))(embedded)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x = keras.layers.Dense(64, activation='relu')(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYIpOKjFk2SU"
      },
      "source": [
        "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              optimizer=keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YluqzM-kk29d"
      },
      "source": [
        "model.fit(int_train_ds, \n",
        "          epochs=5,\n",
        "          validation_data=int_val_ds,\n",
        "          callbacks=[save_best_model('best_checkpoint_1_stackedlstm')]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoGvifEkr4YJ"
      },
      "source": [
        "model.load_weights(\"best_checkpoint_1_stackedlstm\")\n",
        "model.evaluate(int_test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pimDxqhrem-"
      },
      "source": [
        "As before, we prepare the model for deployment by adding in vectorization layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zGairo5qbbW"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = vectorize_layer(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "inference_model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              optimizer=keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmt-SB9byQaK"
      },
      "source": [
        "inference_model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrUj22o7lDhc"
      },
      "source": [
        "sample_text = \"I kind of dozed off halfway through the show.\"\n",
        "text = tf.convert_to_tensor(sample_text)\n",
        "text = tf.expand_dims(text, axis=0)\n",
        "#sample_text = \"This movie has to be the worst I have seen. \"\n",
        "pred = inference_model(text, training=False)[0]\n",
        "if pred >= 0.5: \n",
        "    print(f'positive sentiment: {pred}')\n",
        "else:\n",
        "    print(f'negative sentiment: {pred}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZ8iX8AqtF2"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "For the first model, experiment with any of following to see if you get better or worse validation accuracy.\n",
        "\n",
        "1. Increase/decrease vocabulary size \n",
        "2. Increase/decrease Embedding dimensions \n",
        "3. Use uni-directional LSTM instead of bidirectional LSTM.\n",
        "\n",
        "Provide a plausible explanation to your observation.\n"
      ]
    }
  ]
}