{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-3/2.feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzhOTiHVcQW1"
      },
      "source": [
        "# Transfer learning - Feature Extraction\n",
        "\n",
        "In this exercise, we use transfer learning to improve our baseline model. We will use a pre-trained CNN model as a feature extractor and use the extracted features to train a classifier for our emotion classification task.\n",
        "\n",
        "At the end of this exercise, you will be able to: \n",
        "- understand how to load a pretrained model with and without the classification layer  \n",
        "- extract features using the pre-trained model as feature extractor\n",
        "- train a classifier using the extracted features \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE_WxnF6cQW2"
      },
      "source": [
        "Transfer learning involved using the \"knowledge\" learnt from another task (e.g. doing image classification on a large dataset such as ImageNet) and transfer that knowledge to a new and related task (e.g doing image classification on different types of objects than the original ones or for doing object detection). There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. Let's start with feature extraction approach. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhhojEoUcQW2"
      },
      "source": [
        "## Feature extraction\n",
        "\n",
        "In this approach, we only take the convolutional base of a pretrained model and use it to extract features from the images, and use the extracted features as input features to train a separate classifier. \n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/resources/swapping_fc_classifier.png\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YwEl2QzcQW3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and validation dataset\n",
        "Let's go ahead and prepare our train and validation dataset as before."
      ],
      "metadata": {
        "id": "xfudf-b0A_XW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i67_iydk_WcE"
      },
      "outputs": [],
      "source": [
        "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
        "path_to_zip = tf.keras.utils.get_file(origin=dataset_url, extract=True, cache_dir='.')\n",
        "dataset_folder = os.path.dirname(path_to_zip)\n",
        "dataset_folder = os.path.join(dataset_folder, 'flower_photos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMiNBO5Z_WcF"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "image_size = (128,128)\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int'\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(val_ds.class_names)"
      ],
      "metadata": {
        "id": "7NDUjXo1C6ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-jVRqR4cQW4"
      },
      "source": [
        "## Using pre-trained Model as Feature Extractor\n",
        "\n",
        "Let's use EfficientNetB0 as our pretrained model (you can choose any other pretrained model, such as VGG19, ResNet, etc). In the following code, we load the model EfficientNetB0 without including the classification layers (`include_top=False`). In the weights, we specify that we want to download the weights that was trained on ImageNet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOOmRfUJcQW6"
      },
      "outputs": [],
      "source": [
        "# Specify the intended image size we want\n",
        "base_model = keras.applications.efficientnet.EfficientNetB0(input_shape=image_size + (3,),\n",
        "                                      include_top=False,\n",
        "                                      weights='imagenet')\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUCJI-mBcQW7"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "Examine the print out from `model.summary()`\n",
        "- What is the last layer in the pretrained model and what is the output shape? Do you have any Fully connected layers?\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "The last layer is the Conv2D layer. The output is a 1280 feature maps of 4x4 size. There is no Fully connected (Dense) layers. The network is a convolutional base network.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QWlVQdkcQW9"
      },
      "source": [
        "### Extracting features on the train set \n",
        "\n",
        "We will first define a function to perform feature extraction, given an image dataset. \n",
        "\n",
        "We can use `predict()` of the model to loop through all the train images (and also the validation images), or just pass the images directly to the keras model, e.g. `model(images)`. The output will be the features spit out by the convolutional base. We will then use these features as our training samples instead of the original images.\n",
        "\n",
        "However, before we pass the images through the convolutional base, it is IMPORTANT to pre-process the image using the model-specific preprocessing function. Many people *FORGOT* about this step. Different model expect the images to be of specific range of values (e.g. some models expect the pixel values to be between 0 and 1, some between -1 and 1) and specific channel ordering (e.g. VGGNet expects the channel to be BGR). So we need to make sure our images are pre-processed according to what the model expects.\n",
        "\n",
        "**NOTE**: For EfficientNet, the pre-processing is part of the model, so the preprocess_input function is just pass-thru and not necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD7rPV3zcQW9"
      },
      "outputs": [],
      "source": [
        "# retrieve the preprocess_input function of convolutional model for use later\n",
        "# NOTE: For EfficientNet, the pre-processing is part of the model, so the preprocess_input function is just pass-thru\n",
        "preprocess_input_fn = keras.applications.efficientnet.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90bXhWHcQW-"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = False\n",
        "\n",
        "def get_features_labels(dataset): \n",
        "\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in dataset:   # each iteration yields a batch of images\n",
        "        # pre-process the features\n",
        "        preprocessed_images = preprocess_input_fn(images)\n",
        "        features = base_model(preprocessed_images)\n",
        "        \n",
        "        # append the batch of features to all_features and all_labels\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    # concatenate the features from all the batches\n",
        "    all_features, all_labels = np.concatenate(all_features), np.concatenate(all_labels)\n",
        "    \n",
        "    return all_features, all_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijgudSr7cQW-"
      },
      "source": [
        "Now we will call the extract function on both training dataset and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb5ThObpcQW-"
      },
      "outputs": [],
      "source": [
        "# Extract features and labels for train set\n",
        "X_train, y_train = get_features_labels(train_ds)\n",
        "\n",
        "# Extract features and labels for validation set\n",
        "X_val, y_val = get_features_labels(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNQvgD4EcQW-"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the features\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZrc_5cVcQW_"
      },
      "source": [
        "We will now save the features to local storage, as numpy arrays. We will load these features later on to be used for training our classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CirH4sWcQW_"
      },
      "outputs": [],
      "source": [
        "np.save(\"X_train.npy\", X_train)\n",
        "np.save(\"y_train.npy\", y_train)\n",
        "np.save(\"X_val.npy\", X_val)\n",
        "np.save(\"y_val.npy\", y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKldlOhOcQW_"
      },
      "source": [
        "## Classification model\n",
        "\n",
        "Now we will build a new classification model that takes in the extracted features as input. Instead of the usual flatten layer, followed by dense layers, let us use a GAP layer, followed by Dense (with 256 units), a Dropout (with 50%) and another Dense that output the prediction. Compile your model using Adam with a learning rate of 0.001.\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "1. What should be input shape to our model? \n",
        "2. What is the output shape of the Global Average Pooling (GAP) layer? \n",
        "3. How many units we need for output, and what should we use as activation function? \n",
        "\n",
        "Complete the code below. \n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "1. The input shape should be (4, 4, 1280) which is the output shape of our convolutional base\n",
        "2. The output shape of GAP is (1280) since the maxpooling layer (the last layer) of the convolutional base has 1280 feature maps (channels). \n",
        "3. We need  5 output units as we are classifying 5 different flowers and we should use 'softmax' as the activation function for multi-class classification. \n",
        "\n",
        "Codes: \n",
        "\n",
        "```python\n",
        "inputs = keras.layers.Input(shape=X_train.shape[1:])\n",
        "x = keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "x = keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "outputs = keras.layers.Dense(units=5, activation=\"softmax\")(x)\n",
        "\n",
        "model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
        "\n",
        "model_top.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "``` \n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trMqDpjPcQW_"
      },
      "outputs": [],
      "source": [
        "# TODO: build your classification model here, try to use functional API to do so.\n",
        "\n",
        "inputs = ??\n",
        "\n",
        "## any other layers\n",
        "\n",
        "outputs = ??\n",
        "\n",
        "model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
        "\n",
        "model_top.compile(loss=??, \n",
        "                  optimizer=??, \n",
        "                  metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oOdyJjYcQW_"
      },
      "outputs": [],
      "source": [
        "model_top.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVJWxOVgcQXA"
      },
      "source": [
        "Now we train our classifier we the extracted features (X_train) for 30 epochs. The training will be fast, as we only have very few parameters (around 200k) to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBd_NrImcQXA"
      },
      "outputs": [],
      "source": [
        "# we will now load the extracted features from the files we save to earlier\n",
        "X_train = np.load('X_train.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "X_val = np.load('X_val.npy')\n",
        "y_val = np.load('y_val.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eczFKxhZm6ON"
      },
      "outputs": [],
      "source": [
        "# create the tensorboard callback\n",
        "import os\n",
        "import time\n",
        "\n",
        "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "def get_run_logdir():    # use a new directory for each run\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "tb_callback = keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "# create model checkpoint callback to save the best model checkpoint\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKqD1T86cQXA"
      },
      "outputs": [],
      "source": [
        "model_top.fit(X_train, y_train, \n",
        "              epochs=50, \n",
        "              batch_size=16,\n",
        "              validation_data=(X_val, y_val), \n",
        "              callbacks=[tb_callback, model_checkpoint_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwMQWsPfn8Wf"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOqWHsbXoN3K"
      },
      "source": [
        "Let's load the best-performing model checkpoints and use it to compute classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPP1F8f4cQXA"
      },
      "outputs": [],
      "source": [
        "model_top.load_weights('best_checkpoint')\n",
        "model_top.evaluate(X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bs-21XRcQXB"
      },
      "source": [
        "You should see an good improvement in the model, as compared to the previous model. The model also takes much less time to train. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqrdAJdwcQXB"
      },
      "source": [
        "## Prepare the model for deployment\n",
        "\n",
        "We cannot use our `model_top` directly for image classification, as it take extracted features as input, not images. We need to stick back our convolutional base that can take in images directly. This is what we are going to do below. It is also important to include the model-specific pre-processing function as one of the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIU3MBx7cQXB"
      },
      "outputs": [],
      "source": [
        "# specify the input layer with appropriate image shape\n",
        "inputs = keras.layers.Input(shape=image_size+(3,))\n",
        "\n",
        "# important to include model-specific preprocess function\n",
        "x = preprocess_input_fn(inputs)\n",
        "\n",
        "x = base_model(x)\n",
        "outputs = model_top(x)\n",
        "\n",
        "model_full = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "model_full.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "model_full.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGf_nY58cQXC"
      },
      "outputs": [],
      "source": [
        "model_full.save(\"full_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ciRBIOgqtQ5"
      },
      "source": [
        "Let's make sure our full model works on the validation dataset (which are images) and gives the same accuracy as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaBScJj3cQXC"
      },
      "outputs": [],
      "source": [
        "restored_model = tf.keras.models.load_model('full_model')\n",
        "restored_model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKxz3wUIcQXD"
      },
      "source": [
        "## Extra exercises\n",
        "\n",
        "Try another pre-trained model such as VGG19 or ResNet50 and see if the extracted features give you better classification result. \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}