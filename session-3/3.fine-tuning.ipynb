{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-3/3.fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce6b8183-8ef0-4664-813f-6a635bda0f15",
      "metadata": {
        "id": "ce6b8183-8ef0-4664-813f-6a635bda0f15"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Another widely used transfer learning technique is _fine-tuning_. \n",
        "Fine-tuning involves unfreezing a few of the top layers \n",
        "of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in our case, the \n",
        "fully-connected classifier) and these unfrozen top layers. This is called \"fine-tuning\" because it slightly adjusts the more abstract \n",
        "representations of the model being reused, in order to make them more relevant for the problem at hand.\n",
        "\n",
        "\n",
        "\n",
        "![fine-tuning VGG16](https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/resources/vgg16_fine_tuning.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768ead86-19a3-4908-be5b-57538c555a19",
      "metadata": {
        "id": "768ead86-19a3-4908-be5b-57538c555a19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35f9d10-815d-4b80-9cc5-b6c29cd2010f",
      "metadata": {
        "id": "a35f9d10-815d-4b80-9cc5-b6c29cd2010f"
      },
      "source": [
        "## Creating Datasets\n",
        "\n",
        "We will setup our training and validation dataset as we did in earlier exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8d2e8a-b80a-410a-afb7-b7534a83af77",
      "metadata": {
        "id": "dc8d2e8a-b80a-410a-afb7-b7534a83af77"
      },
      "outputs": [],
      "source": [
        "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
        "path_to_zip = tf.keras.utils.get_file(origin=dataset_url, extract=True, cache_dir='.')\n",
        "dataset_folder = os.path.dirname(path_to_zip)\n",
        "dataset_folder = os.path.join(dataset_folder, 'flower_photos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208febe1-3fdf-41e3-999c-3fac7e70f007",
      "metadata": {
        "id": "208febe1-3fdf-41e3-999c-3fac7e70f007"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "image_size = (128,128)\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int'\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(val_ds.class_names)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "L2bkURuBHjl2"
      },
      "id": "L2bkURuBHjl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b22f27d8-f9c2-4168-969c-2b5c59eccb35",
      "metadata": {
        "id": "b22f27d8-f9c2-4168-969c-2b5c59eccb35"
      },
      "source": [
        "## Transfer Learning Workflow \n",
        "\n",
        "It is necessary to freeze the convolution base before training a randomly initialized classifier top. If the classifier wasn't already trained, then the error signal propagating through the network during training would be too large, and the representations previously learned by the layers being fine-tuned would be destroyed. Thus the steps for fine-tuning a network are as follow:\n",
        "\n",
        "1. Add your custom network on top of an already trained base network.\n",
        "2. Freeze the convolutional base network.\n",
        "3. Train the classification top you added.\n",
        "4. Unfreeze some layers in the base network.\n",
        "5. Jointly train both these layers and the part you added.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cdeee29-8fee-4f03-93c9-8257f28403a1",
      "metadata": {
        "id": "3cdeee29-8fee-4f03-93c9-8257f28403a1"
      },
      "source": [
        "#### BatchNormalization layer \n",
        "\n",
        "Many CNN models contain BatchNormalization layers. \n",
        "BatchNormalization contains 2 non-trainable variables that keep track of the mean and variance of the inputs. These variables are updated during training time. Here are a few things to note when fine-tuning model with BatchNormalization layers: \n",
        "- When you set `bn_layer.trainable = False`, the BatchNormalization layer will run in inference mode, and will not update its mean & variance statistics. \n",
        "- When you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing `training=False` when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0d8765-9894-422a-8c70-85b023517f86",
      "metadata": {
        "id": "dd0d8765-9894-422a-8c70-85b023517f86"
      },
      "source": [
        "## Build our Model \n",
        "\n",
        "We will now construct our model: a convolutional base (initialized with pre-trained weights) and our own classification head (initialized with random weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7689d158-edd7-4015-9ce8-29fc8f87ecfc",
      "metadata": {
        "id": "7689d158-edd7-4015-9ce8-29fc8f87ecfc"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.RandomRotation(0.1),\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\")\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RB9VSinUyvhV",
      "metadata": {
        "id": "RB9VSinUyvhV"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model \n",
        "base_model = keras.applications.EfficientNetB0(input_shape=image_size + (3,),\n",
        "                                         include_top=False,\n",
        "                                         weights='imagenet')\n",
        "\n",
        "## This is not necessary as it is just a passthrough. EfficientNet model includes the rescaling layer that preprocess the input\n",
        "## refer to https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/preprocess_input\n",
        "preprocess_input_fn = keras.applications.efficientnet.preprocess_input\n",
        "\n",
        "# freeze the base layer \n",
        "base_model.trainable = False\n",
        "\n",
        "# Add input layer \n",
        "inputs = keras.layers.Input(shape=image_size+(3,))\n",
        "\n",
        "x = data_augmentation(inputs)\n",
        "# Add preprocessing layer\n",
        "\n",
        "## This is not necessary as it is just a passthrough. EfficientNet model includes the rescaling layer that preprocess the input\n",
        "## refer to https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/preprocess_input\n",
        "x = preprocess_input_fn(x)\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "# Add our classification head\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "x = keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "outputs = keras.layers.Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=base_learning_rate), \n",
        "                  metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0UVNujET2sho",
      "metadata": {
        "id": "0UVNujET2sho"
      },
      "source": [
        "Let's confirm all the layers of convolutional base are frozen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c15c81-d461-4881-b6c8-ae948e2361ad",
      "metadata": {
        "id": "42c15c81-d461-4881-b6c8-ae948e2361ad"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    print(f'layer name = {layer.name}, trainable={layer.trainable}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q418z8cI3V-O",
      "metadata": {
        "id": "Q418z8cI3V-O"
      },
      "source": [
        "Let's print out the model summary and see how many trainable weights. We can see that we only 329,221 trainable weights (parameters), coming from the classification head that put on top of the convolutional base. (For comparison, a EfficientNetB0 has total of 4,049,571 weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d8db2b-5e00-4a3f-97e4-bf68cd138b60",
      "metadata": {
        "id": "a3d8db2b-5e00-4a3f-97e4-bf68cd138b60"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZSyhl8et47AV",
      "metadata": {
        "id": "ZSyhl8et47AV"
      },
      "source": [
        "## Train the classification head \n",
        "\n",
        "We will go ahead and train our classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cqSDQ4gP5HEO",
      "metadata": {
        "id": "cqSDQ4gP5HEO"
      },
      "outputs": [],
      "source": [
        "# create model checkpoint callback to save the best model checkpoint\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, \n",
        "          epochs=50, callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xjZaiOhQH8yy",
      "metadata": {
        "id": "xjZaiOhQH8yy"
      },
      "outputs": [],
      "source": [
        "model.load_weights('best_checkpoint')\n",
        "model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172d426c-7666-47b1-84a6-485826b79641",
      "metadata": {
        "id": "172d426c-7666-47b1-84a6-485826b79641"
      },
      "source": [
        "Now we have our classification layers trained, let's start to unfreeze some top layers of the convolutional base to fine tune the weights. \n",
        "Let's try to fine-tune the last convolutional blocks (i.e. from `block7a` onwards)\n",
        "\n",
        "Why not fine-tune more layers? Why not fine-tune the entire convolutional base? We could. However, we need to consider that:\n",
        "\n",
        "* Earlier layers in the convolutional base encode more generic, reusable features, while layers higher up encode more specialized features. It is \n",
        "more useful to fine-tune the more specialized features, as these are the ones that need to be repurposed on our new problem. There would \n",
        "be fast-decreasing returns in fine-tuning lower layers.\n",
        "* The more parameters we are training, the more we are at risk of overfitting. The convolutional base has 4M parameters, so it would be \n",
        "risky to attempt to train it on our small dataset.\n",
        "\n",
        "Thus, in our situation, it is a good strategy to try to fine-tune the few layers in the convolutional base.\n",
        "\n",
        "Let's set this up, we will unfreeze our `base_model`, \n",
        "and then freeze individual layers inside of it, except the block7a onwards. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, layer in enumerate(base_model.layers): \n",
        "    if layer.name == 'block7a_expand_conv': \n",
        "        break\n",
        "print(idx)"
      ],
      "metadata": {
        "id": "80OOvVrAKntZ"
      },
      "id": "80OOvVrAKntZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690283a8-4c53-4293-b8f2-c0685d8031aa",
      "metadata": {
        "id": "690283a8-4c53-4293-b8f2-c0685d8031aa"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:idx]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c83f93-c35c-44c5-9925-f24191c917ce",
      "metadata": {
        "id": "e4c83f93-c35c-44c5-9925-f24191c917ce"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    print(layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wWXyo3n5903l",
      "metadata": {
        "id": "wWXyo3n5903l"
      },
      "source": [
        "Let us examine model summary again. We can see now that we have more trainable weights 1,458,613 compared to previously 329,221."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8uq86nMv-HxP",
      "metadata": {
        "id": "8uq86nMv-HxP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdZPk1Ay80r_",
      "metadata": {
        "id": "bdZPk1Ay80r_"
      },
      "source": [
        "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage as we do not want to make too drastic changes to the weights in the convolutional layers under fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc1da460-3c8a-40f3-a9e5-14c7b832ef79",
      "metadata": {
        "id": "bc1da460-3c8a-40f3-a9e5-14c7b832ef79"
      },
      "outputs": [],
      "source": [
        "finetune_learning_rate = base_learning_rate / 10.\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=finetune_learning_rate),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_finetune_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=20,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2496b41f-63bd-42ca-aa6a-923a6588d4d8",
      "metadata": {
        "id": "2496b41f-63bd-42ca-aa6a-923a6588d4d8"
      },
      "outputs": [],
      "source": [
        "model.load_weights('best_finetune_checkpoint')\n",
        "model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RKt9lTt3_qYi",
      "metadata": {
        "id": "RKt9lTt3_qYi"
      },
      "source": [
        "**Question:**\n",
        "\n",
        "Is our fine-tuned model performing better or worse than the previous model?\n",
        "\n",
        "Provide a possible explanation to your observation. \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3.fine-tuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}