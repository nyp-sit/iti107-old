{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-2/using_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2hCMeGcCDYu"
   },
   "source": [
    "# Using Pretrained CNN models\n",
    "\n",
    "Welcome to this week's programming exercise. We have covered many different Convolutional Neural Network architectures such as VGG, ResNet, Inception and MobileNet. It is time to see them in action. \n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- load pretrained models of some popular Convolutional Neural Networks and use them to classify images\n",
    "- identify some of the architecture patterns in the popular Convolutional Neural Network\n",
    "- compare the inference speed of different models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef9t8CVTCDYv"
   },
   "source": [
    "## Get the sample image\n",
    "\n",
    "We will use the pretrained model to classify a sample image (a picture of table and chair). Let's go ahead and download the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myEjNojPCDYv"
   },
   "outputs": [],
   "source": [
    "# wget is a linux command available on linux os like Ubuntu\n",
    "!wget --no-check-certificate https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/resources/chair_table.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6_OEul4CDYw"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAfkrlODCDYw"
   },
   "outputs": [],
   "source": [
    "# Read Images \n",
    "img_path = 'chair_table.png'\n",
    "img = tf.keras.utils.load_img(img_path)\n",
    "  \n",
    "# display Images \n",
    "plt.imshow(img) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmWNmNIaCDYx"
   },
   "source": [
    "## VGG16 - Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJK7r_MfCDYx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "vgg16_model = vgg16.VGG16(weights='imagenet')\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIXP4z_kCDYy"
   },
   "source": [
    "***Questions***\n",
    "\n",
    "1. What is the expected input image size?\n",
    "2. What are the last four layers in VGG-16? \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. it is expected to have a height of 224 and width of 224\n",
    "2. the last 4 layers are flatten (which flattens the 2-D array into 1-D array before feeding to FC layer), and 2 Fully-connected (Dense) layers, and the last layer is a soft-max layer to classify 1000-classes. This is quite typical of a image classifier.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmLlX0D9CDYy"
   },
   "outputs": [],
   "source": [
    "# Utility Function to Load Image, Preprocess input and Targets\n",
    "def predict_image(model, img_path, preprocess_input_fn, decode_predictions_fn, target_size=(224,224)):\n",
    "    img = tf.keras.utils.load_img(img_path, target_size=target_size)\n",
    "    x = tf.keras.utils.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input_fn(x)\n",
    "    preds = model.predict(x)\n",
    "    predictions_df = pd.DataFrame(decode_predictions_fn(preds, top=10)[0])\n",
    "    predictions_df.columns = [\"Predicted Class\", \"Name\", \"Probability\"]\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWbhoYU-CDYy"
   },
   "outputs": [],
   "source": [
    "# Predict Results\n",
    "predict_image(vgg16_model, img_path, vgg16.preprocess_input, vgg16.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLCHDf5ZCDYz"
   },
   "source": [
    "Notice that we pass in `vgg.preprocess_input` function to preprocess the image before calling `model.predict()`. Different network (e.g. VGG, ResNet, etc) expects the input image to be normalized in different ways, and different models will provide their own preprocess_input() function to perform the normalization.\n",
    "\n",
    "We also call `np.expand_dims(x, axis=0)` before calling `preprocess_input()` and `predict()`. \n",
    "\n",
    "***Question***\n",
    "\n",
    "1. What does `np.expand_dims(x, axis=0)` do and why do we need it? \n",
    "2. Our sample picture consists of both table and chair? What does VGG16 predict? and why do you think it predicts so?\n",
    "3. Of the top 10 predictions, did you see any prediction about chair? \n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "1. np.expand_dims() increases the number of dimensions and the axis of the new dimension is specified by the axis parameter. In this case, we add in a new axis as axis=0, first axis. This is because the preprocess_input() and predict() function expects the images to be in the shape (samples, height, width, channels), the 1st axis being the batch.\n",
    "\n",
    "2. It predicts dining table. It probably focus on the object in the middle of the image.\n",
    "\n",
    "3. Yes, folder chair is one of the top 10 predictions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8ApK74KCDYz"
   },
   "source": [
    "## Resnet50 - Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaVZqpQtCDYz"
   },
   "outputs": [],
   "source": [
    "# It will download the weights that might take a while\n",
    "# Also, the summary will be quite long, since Resnet50 is a much larger network than VGG16\n",
    "from tensorflow.keras.applications import resnet50\n",
    "\n",
    "resnet50_model = resnet50.ResNet50(weights='imagenet')\n",
    "\n",
    "# let's plot the model, instead of using model.summary(), as it is easier to see some of the skip connections\n",
    "tf.keras.utils.plot_model(resnet50_model, to_file=\"resnet.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_OCcrNqCDY0"
   },
   "source": [
    "***Questions***\n",
    "\n",
    "1. Can you identify the skip connection block from the model plot()?\n",
    "2. Look at the last few layers in the ResNet. How are they different from those of VGG-16?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. Look for those 'Add' layer (e.g. layer with name add_2). The Add layer adds the skip connection with the previous layer. Notice that the add is done before the Activation function. You can also call plot_model() to get a graphical visualization of the model.\n",
    "\n",
    "2. ResNet does not use make use of Full-connected layers as classification layers. Instead it replaces the FC layers with GlobalAveragePooling2D. This architecture is very common in more modern architectures.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x364WqMyCDY1"
   },
   "outputs": [],
   "source": [
    "# Predict Results\n",
    "predict_image(resnet50_model, img_path, resnet50.preprocess_input, resnet50.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcO8nn66CDY1"
   },
   "source": [
    "## MobileNet v2 - Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSjnAiBPCDY1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "mobilenet_v2_model = mobilenet_v2.MobileNetV2(weights='imagenet')\n",
    "\n",
    "# print the model summary\n",
    "tf.keras.utils.plot_model(mobilenet_v2_model, to_file='mobilenet_v2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht8Rgd-SCDY1"
   },
   "source": [
    "***Questions***\n",
    "\n",
    "1. Can you identify the Depth-wise Convolution layer from the model summary?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "1. For example, the layer called 'block_1_depthwise'. \n",
    "\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msTztl9wCDY2"
   },
   "outputs": [],
   "source": [
    "predict_image(mobilenet_v2_model, img_path, mobilenet_v2.preprocess_input, mobilenet_v2.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCicGOVQCDY2"
   },
   "source": [
    "### Speed comparison \n",
    "\n",
    "We compare the inference speed of the three different models. Which one has the fastest inference speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(img_path, target_size=(224,224)) \n",
    "img_arr = tf.keras.utils.img_to_array(img)\n",
    "img_arr = np.expand_dims(img_arr, axis=0)\n",
    "\n",
    "# duplicate the image 128 times so that we can see significant differences between different models\n",
    "images = tf.repeat(img_arr, repeats=[128], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PokbI6HZCDY2"
   },
   "outputs": [],
   "source": [
    "processed_image = vgg16.preprocess_input(images)\n",
    "%timeit vgg16_model.predict(processed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eT88wFAnCDY3"
   },
   "outputs": [],
   "source": [
    "processed_image = resnet50.preprocess_input(images)\n",
    "%timeit resnet50_model.predict(processed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UG3ceEDdCDY3"
   },
   "outputs": [],
   "source": [
    "processed_image = mobilenet_v2.preprocess_input(images)\n",
    "%timeit mobilenet_v2_model.predict(processed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "using_pretrained_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
