{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-1/first_cnn_for_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "# First Convolutional Neural Network for Image Classification\n",
        "\n",
        "In this exercise, you will learn to build your first simple Convolutional Neural Network and use it to classify images. \n",
        "\n",
        "You will learn: \n",
        "- how to construct a Convolutional Neural Networks \n",
        "- adjust the different hyper-parameters of the network (e.g. number of filters, number of layers, etc) and observe the effects \n",
        "- how to visualize the activations of the hidden layers \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQmFSPe7Wm8d"
      },
      "source": [
        "## Fashion MNIST Dataset\n",
        "\n",
        "We will be using a toy dataset [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. \n",
        "\n",
        "![fashion-mnist](https://github.com/nyp-sit/sdaai-iti107/blob/main/session-1/images/fashion-mnist.png?raw=1)\n",
        "\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "|Label|Class|\n",
        "|---|---|\n",
        "|0|T-shirt/top|\n",
        "|1|Trouser|\n",
        "|2|Pullover|\n",
        "|3|Dress|\n",
        "|4|Coat|\n",
        "|5|Sandal|\n",
        "|6|Shirt|\n",
        "|7|Sneaker|\n",
        "|8|Bag|\n",
        "|9|Ankle boot|       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3XzIfPkWm8e"
      },
      "source": [
        "Let's load the data using `keras.datasets` as it is part of datasets available from keras.\n",
        "For a list of dataset available from keras, see https://www.tensorflow.org/api_docs/python/tf/keras/datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "QzmOozJpUK3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0tFgT1MMKi6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (validation_images, validation_labels) = mnist.load_data()\n",
        "print('Shape of training_images = {}'.format(training_images.shape))\n",
        "print('Shape of validation_images = {}'.format(validation_images.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvK-o6CanZur"
      },
      "source": [
        "Note that the data is in numpy arrays and not tensor. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHLv_hwNnZur"
      },
      "outputs": [],
      "source": [
        "print(type(training_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6JrBRwWm8g"
      },
      "source": [
        "## Preprocess the images\n",
        "\n",
        "You need to preprocess the image before using it as the input to the CNN.\n",
        "CNN expects our input to be of the shape (batch, height, width, channels). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MI9zUqGWm8h"
      },
      "outputs": [],
      "source": [
        "# reshape to a 4-D tensors, with number of channel as 1, since this is a gray scale image\n",
        "training_images = np.expand_dims(training_images, axis=3)\n",
        "validation_images = np.expand_dims(validation_images, axis=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om3eCKWbWm8i"
      },
      "source": [
        "## Build your first CNN\n",
        "\n",
        "A typical CNN consists of 1 or more blocks of Conv2D layer followed by MaxPooling2D layer. The 2D array from the last convolutional block will then be flattened into 1D array before feeding into Dense (fully connected) layer for classification. The last layer uses `softmax` to ouput the probabilities of each of the 10-classes. Note that the last layer has to have same number of output units as the number of classes (in our case, we have 10 classes, so we need 10 output units). \n",
        "\n",
        "The original image is in the range (0,255). Neural network will learn better if the input values are normalized to between (0.0, 1.0). We have added in a keras layer Rescaling() to scale the input values to between 0 and 1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "822gPjDAWm8k"
      },
      "outputs": [],
      "source": [
        "def make_model(input_shape, num_classes):\n",
        "\n",
        "    # define the input layer with appropriate shape\n",
        "    inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "    x = keras.layers.Rescaling(scale=1./255)(inputs)\n",
        "    x = keras.layers.Conv2D(32, 3, activation='relu', name='conv1')(x)\n",
        "    x = keras.layers.MaxPooling2D(2, name='pool1')(x)\n",
        "    x = keras.layers.Conv2D(64, 3, activation='relu', name='conv2')(x)\n",
        "    x = keras.layers.MaxPooling2D(2, name='pool2')(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(128, activation='relu', name='dense1')(x)\n",
        "\n",
        "    if num_classes > 2: \n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    else: \n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "\n",
        "    outputs = keras.layers.Dense(units, activation=activation, name='dense2')(x)\n",
        "    \n",
        "    return keras.Model(inputs, outputs)\n",
        "        \n",
        "\n",
        "# call make_model with appropriate argument values (shape and num_classes)\n",
        "model = make_model((28,28,1), 10)\n",
        "\n",
        "# compile your model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZkj2gTgByXv"
      },
      "source": [
        "Look at the model summary carefully and make sure you understand why the output shape is as shown and also how to calculate the number of parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTtXgCWGB2Mc"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGllb4wvWm8l"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Let's first define a convenience method to create a Tensorboard callback to log the training events. We will also create a ModelCheckpoint callback to save the best-performing set of weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn37WijtYJKa"
      },
      "outputs": [],
      "source": [
        "def create_tb_callback(): \n",
        "\n",
        "    import os\n",
        "    \n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "        \n",
        "        import time\n",
        "        \n",
        "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "        \n",
        "        return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eYLfKh2Wm8m"
      },
      "outputs": [],
      "source": [
        "model.fit(training_images, \n",
        "          training_labels, \n",
        "          batch_size=256, \n",
        "          epochs=30,\n",
        "          validation_data=(validation_images, validation_labels),\n",
        "          callbacks=[create_tb_callback(), model_checkpoint_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAKh85YI8ima"
      },
      "outputs": [],
      "source": [
        "model.load_weights('best_checkpoint')\n",
        "model.evaluate(validation_images, validation_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo9XVvMPWm8m"
      },
      "source": [
        "## Visualize the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSR5hMLsWm8n"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tb_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oqAEB5uWm8n"
      },
      "source": [
        "We can see that model achieves training accuracy of 98% but the validation accuray stagnates at 92%. So there is some overfitting here. You can try to improve the model by adding in some regularization such as Dropout layer, etc. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "## Visualizing the Convolutions and Pooling\n",
        "\n",
        "It is often said that deep learning network is a blackbox. However, this is certainly not true for Convnets. The representations learnt by Convnets are highly interpretable, as they are representations of visual concepts. \n",
        "\n",
        "The following codes allows us to visualize the output of the feature maps learnt by Convnet. By looking at output (activations) of these feature maps, for different kind of images, we will understand how a specific image is being classified. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daeA1dblWm8o"
      },
      "source": [
        "Let's first print out the labels of the first 10 test labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwWN7jA5Wm8o"
      },
      "outputs": [],
      "source": [
        "print(validation_labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMvVVghVWm8p"
      },
      "source": [
        "Let us look two different images, image 0 with label 9 (ankle boot) and image 2 with label 1 (trouser)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-6nX4QsOku6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "ANKLE_BOOT_IDX = 0\n",
        "TROUSER_IDX = 2\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "ax1.imshow(validation_images[ANKLE_BOOT_IDX].reshape(28,28))\n",
        "ax2.imshow(validation_images[TROUSER_IDX].reshape(28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yib6Ral5Wm8p"
      },
      "source": [
        "Let's create activation model for each individual layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0q7lfUOWm8q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pprint\n",
        "\n",
        "# extract the outputs of layer 2 to  layer 5 (only the Conv2D, MaxPooling2D layers)\n",
        "layer_outputs = [layer.output for layer in model.layers][2:6]\n",
        "pprint.pprint(layer_outputs)\n",
        "\n",
        "# create activation models that will return these outputs given the model input\n",
        "activation_model_conv1 = keras.Model(inputs=model.input, outputs=layer_outputs[0])\n",
        "activation_model_pool1 = keras.Model(inputs=model.input, outputs=layer_outputs[1])\n",
        "activation_model_conv2 = keras.Model(inputs=model.input, outputs=layer_outputs[2])\n",
        "activation_model_pool2 = keras.Model(inputs=model.input, outputs=layer_outputs[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytiBpiOWm8q"
      },
      "source": [
        "Let's look at activations from the 1st Conv2D layer for both images. There are 32 filter maps from the 1st Conv layer, but we going to look at only the first 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDoMaVz5Wm8q"
      },
      "outputs": [],
      "source": [
        "fig, axarr = plt.subplots(2, 10, figsize=(20, 4))\n",
        "ankle_boot_activations_conv1 = activation_model_conv1.predict(validation_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
        "trouser_activations_conv1 = activation_model_conv1.predict(validation_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
        "\n",
        "for filter_idx in range(0, 10):\n",
        "    axarr[0, filter_idx].imshow(ankle_boot_activations_conv1[0,:,:, filter_idx])\n",
        "    axarr[1, filter_idx].imshow(trouser_activations_conv1[0,:,:,filter_idx])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ADN_PUWm8r"
      },
      "source": [
        "From the plots, we can see that 1st Conv layer seems to act as detector of lines and edges. Some filters act more like vertical line detectors, whereas some filters detect edges of the shape.\n",
        "\n",
        "Your filter output may not be the same as we have shown here as the specific filters learnt by the Conv layer are not deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izo0O4hHWm8r"
      },
      "source": [
        "Now let's examine the activations from the 2nd Convolutional layer. Again we will only display the output from the first 10 filters.\n",
        "\n",
        "You will observe that the outputs seems to be more abstract and seems to detect a higher-level construct, such a the presence of certain part of the object (e.g. the collar part of the boot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQzcDmZ2Wm8r"
      },
      "outputs": [],
      "source": [
        "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
        "\n",
        "ankle_boot_activations_conv2 = activation_model_conv2.predict(validation_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
        "trouser_activations_conv2 = activation_model_conv2.predict(validation_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
        "\n",
        "for filter_idx in range(0, 10):\n",
        "    axarr[0, filter_idx].imshow(ankle_boot_activations_conv2[0,:,:, filter_idx])\n",
        "    axarr[1, filter_idx].imshow(trouser_activations_conv2[0,:,:,filter_idx])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6y26nDnWm8s"
      },
      "source": [
        "Now, let's examine the activations from the last max-pooling layer for both images. We will just display the first 10.  What do you observe?\n",
        "\n",
        "The MaxPooling2D just highlight or emphasize more sharply the abstract part detected by the Conv layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRY3v4GcWm8s"
      },
      "outputs": [],
      "source": [
        "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
        "\n",
        "ankle_boot_activations_pool2 = activation_model_pool2.predict(validation_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
        "trouser_activations_pool2 = activation_model_pool2.predict(validation_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
        "\n",
        "for filter_idx in range(0, 10):\n",
        "    axarr[0, filter_idx].imshow(ankle_boot_activations_pool2[0,:,:, filter_idx])\n",
        "    axarr[1, filter_idx].imshow(trouser_activations_pool2[0,:,:,filter_idx])\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "First CNN for Image Classification",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}